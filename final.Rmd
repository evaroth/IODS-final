In this assignment I am going to fit a multiple regression model on the dataset human and I will try to make predictions from it.
Furthermore I will conduct linear discriminant analysis (LDA) with the data, train the LDA-model and try the model on a testing data set.

#1. The dataset "human"
The dataset originates from the [United Nationa Development Programme (UNDP)](http://hdr.undp.org/en/content/human-development-index-hdi). The UNDP aims to compare the development of countries and regions of the world, including not only growth alone, but many different variables. In this exercise we are using a subset, that I created from the original data. I selected certain variables concerning health and education as well as the Human Development Index (HDI) and the Gross National Income per capita (GNI). Missing values were removed from the dataset. The file of the data wrangling can be found [here](https://github.com/evaroth/IODS-final/blob/master/datawrangling.R). 

The dataset contains 155 observations of 8 variables. The observations are made for 155 countries. Out of the 8 variables GNI and Mat.Mor are integer variables, the others are numerical. In the following list you may see the names of the variables and what kind of information they have recorded.

* **HDI:** human development index
* **Edu2.FM:** ratio of women with at least secondary education to men with at least seconday reducation
* **Life.Exp:** life expectancy at birth
* **Mean.Edu:** mean years of education
* **GNI:** gross national income per capita
* **Mat.Mor:** maternal mortality ratio
* **Ado.Birth:** adolescent birth rate
* **Parli.F:** percentage of female representatives in parliament

I hypothesize, that the variables concerning education (Edu.FM, Mean.Edu, Parli.F) have a strong influence on the variables concerning health (Life.Exp, Mat.Mor, Parli.F). A better education equally for men and women, has eventually a better influence on life expectancy and health condition of a people.

```{r data human}
#At first I am accessing all the packages that I will be using during this exercise
library(GGally)
library(ggplot2)
library(MASS)
library(corrplot)
#Now we read the data
human <- read.csv("/Users/eva-mariaroth/Documents/GitHub/IODS-final/human_.csv", header = TRUE, sep= ",")
#Exploring the data, its structur and dimensions
str(human)
```
```{r summary}
summary(human)
```

I want to take a closer look at the variables and their relationships to each other. In order to do this I can pair the variables and see if there are correlations.The plot matrix shows us the distribution of the data for each variable. The variables are paired and the results are shown graphically in scatter plots and with the Correlation Coefficient. We can find the highest correlation between Life Expectancy and HDI with a correlation coefficient of 0.905.
```{r human pairs}
p <- ggpairs(human, mapping = aes(col = "green4", alpha = 0.9), lower = list(combo = wrap("facethist", bins = 18)), title = "Fig. 1: Graphical overview of the dataset human")
p
```

Another way to get a quick overview about the correlation of the variables is the correlation plot.In the correlation plot the colour indicates what kind of correlation we have:    
red - negative correlation  
blue - positive correlation  
The size of the cirlce and the intensity of the colour indicate the correlation coefficient.  
strong colours - corrolation coefficient is high    
light colours  - corrolation coefficient is low 
We can see that the the variables are strongly correlating with each other, except for the amount of female representatives in the parliament. It does not seem to have a significant influence on HDI, GNI or Life Expectancy. Obviously other factors play a bigger role here. There are strong positive correlations between HDI, Life Expectancy and Mean Education. Strong negative correlations can be found between HDI and Maternal mortality ratio as well as life expectancy and maternal mortality ratio.

```{r corrplot}
cor_matrix<-cor(human)
corrplot(cor_matrix, method="circle", type = "lower", cl.pos = "b", tl.pos = "d", tl.cex = 0.6, title = "
         Fig.2: Correlation plot for the paired dataset of human")
```

# 2. Regression analysis
##2.1 Simple regression
In order to find out more about the impact of education on health we will perform a regression analysis for the varibales Mean.Edu and Life.Exp, the two variables, very simplified, standing for education and health. In Figure 3 we can see the observations of theses variables plotted against each other and a linear regression line fitted to them. 
```{r simple regr scatterplot}
#creating a scatter plot of Life.Exp versus Mean.Edu with a linear regression line
qplot(Life.Exp, Mean.Edu, data = human, main = "Fig.3: Linear regression for life expectancy and mean years of education", colour = "red") + geom_smooth(method = "lm")
```

Now we want to write a simple regression model and see what kind of information it gives us. The simple regression has the following formula:
![](/Users/eva-mariaroth/Desktop/Bildschirmfoto 2017-12-18 um 11.59.50.png)

Life.Exp is the dependant variable y and Mean.Edu the explanatory variable x. Alpha indicates the point where the regression line intercepts the y-axis and beta to the slope of the regression line.Epsilon is an unobservable random variable, that is assumed to add noise to the observations and describes the errors of the model.
The summary of the simple regression model at first shows us the formula applied, indicating the dependant variable and the explanatory variable. Then it gives us a summary of the residuals of the model. The residuals describe the prediction errors, i.e. the difference between the predicitions and the actual values. The estimate of the intercept describes the alpha parameter. In this case it would be 55.46. The estimate for the variable Mean.Edu gives us the beta parameter: 1,95. The P value of the model is very small, that means that the correlation between the variables is highly significant.

```{r simple regr}
#fitting a linear model where Life.Exp is the target, whereas mean. edu is the explanatory variable
my_model <- lm(Life.Exp ~ Mean.Edu, data = human)
summary(my_model)
```

##2.2 Making predictions using the model
Using the model, that I wrote, we can make predictions, about the life expectancy that we would find in countries, if the mean years of education would change. For this purpose I have added examplarily new values for the mean years of education for six countries. In table 1 we can see the predicted life expectancy that these countries would have, if the mean years of education would correspond to the newly entered values.

```{r predict}
#predicting
m <- lm(Life.Exp ~ Mean.Edu, data = human)

# New observations
new_mean.edu <- c("Niger" = 12.1, "Chad"= 11.5, "Mali" = 10.3, "Nepal" = 10.5, "Haiti" =11.7, "Yemen" = 10)
new_data <- data.frame(Mean.Edu = new_mean.edu)

# Print out the predicted values
predict(m, newdata = new_data)
```

Tab.1:Mean years of education and life expectancy, and the predicted life expectancy if the number of mean years of education would be substantially higher, calculated with the model for six countries. 


Country     | Mean.Edu  | Life.Exp    |Mean.Edu new | Life.exp predicted
----------- | --------  | ----------- | ----------- | ------------
Niger       | 1.5       | 61.4        | 12.1        | 79.1
Chad        | 1.9       | 51.6        | 11.5        | 77.9
Mali        | 2.0       | 58.0        | 10.3        | 75.6
Nepal       | 3.3       | 69.6        | 10.5        | 75.9
Haiti       | 4.9       | 62.8        | 11.7        | 78.3
Yemen       | 2.6       | 63.8        | 10.0        | 74.9


## 2.3 Multiple regression model
For taken more variables into account, we can write a multiple regression model with life expectany beeing the dependant variable and several explanatory variables x1, x2, x3 (Mean.Edu, Edu2.FM, Parli.F). The formula would be:
![](/Users/eva-mariaroth/Desktop/Bildschirmfoto 2017-12-18 um 12.20.24.png)

```{r multipl regr}
#fitting a multiple regression model with several explanatory variables
my_model2 <- lm(Life.Exp ~ Mean.Edu + Edu2.FM + Parli.F, data =human)
summary(my_model2)
```
In the summary of the model we can see, that on the mean years of education have a highly significant correlation with life expectancy. There is no significant correlation between female members in parliament and life expectance.

## 2.4 Validity check for both models
Linear regression models assume that the errors are normally distributed, not correlated and have constant variance, i.e. the size of the error does not depend on the explanatory variables. By analysing the residuals of the model we can explore the validity of the model assumptions.

Below we can see the three diagnostic plots "Residuals versus Fitted", "Normal QQ" and "Residuals vs Leverage" for the simple linear regression . 
The scatter plot of residuals versus fitted proves or disproves the assumption of the constant variance assumption. There is a small tendency of the spread of the residuals to decrease with increasing fitted values. However, the tendancy is not clear. Therefore, i conclude, that the constant variance assumtpion applies.
The QQ plot shows the distribution of the errors. The better the values fit to the dotted line, the more are they normally distributed. We can see that the values fit reasonably well, which means that they are evenly distributed. However, the model seems to be less suitable in both ends of the values. Then there is a small deviation between the line and the errors. 
The leverage measures how much impact a single observation has on the model. In the Residuals versus Leverage plot we can see that no single observation has a 
There are no single outliers and leverage for all the observations is lower then 0.4.
```{r diagnostic2}
par(mfrow = c(2,2))
plot(my_model, which = c(1,2,5))
```


The validation plots for the multiple regression model are very similar to the linear regression. The leverage is even smaller with less then two, though there are two outliers. Both models can be validated. However, this is just an example for the methods and there are a lot of factors that were not considered in this exercise and do also have an impact on life expectancy.
```{r validity}
#exploring validity of the model by drawing diagnostic plots
par(mfrow = c(2,2))
plot(my_model2, which = c(1,2,5))
```


# 3. Performing Linear Discriminant analysis
## 3.1 Preparing the dataset
Liner Discriminant Analysis is a variable reduction technique. The dependant variables is explaned by dimensions instead of variables. The dimensions combine the different variables.
In order to conduct the linear discriminant analysis the dataset needs to be adjusted. I am dropping the HDI and GNI variables from the set as they are not interesting for our research question. Then I am scaling the standard set, using the function scale(), as the data contains only numerical values. The scaling needs to be done, because the linear discriminant analysis assumes that the variables are normally distributed and the variance of each variable is the same. In the scaling process the column means are substracted from the corresponding columns and divided by the standard deviatin. After scaling all the observations are between integral 10 and have a mean of 0.0. Now they can be related to each other and the disctances between the groups can be calculated. 
```{r scaling}
#keep only the columns regarding health and education
human2 <- read.csv("/Users/eva-mariaroth/Documents/GitHub/IODS-final/human2.csv", header = TRUE, sep= ",")
human_scaled <- scale(human2)
# summaries of the scaled variables
summary(human_scaled)
```
Furthermore I am replacing the variable Life.Exp with the categorical variable Lifeexp. The variable was created by splitting the numerical variable into four categories (low, med_low, med_high, high), at 25%, 50% and 75% of the values. Table no. 2 shows how the observations are distributed in the four categories.

Tab. 2: Number of observations in the four categories of variable Lifeexp
```{r class}
# class of the human_scaled object
class(human_scaled)
# change the object to data frame
human_scaled <- as.data.frame(human_scaled)
bins <- quantile(human_scaled$Life.Exp)
bins
#create a categorical variable crime
Lifeexp <- cut(human_scaled$Life.Exp, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
table(Lifeexp)
```
```{r train test.}
#remove original Life.Exp variable from the dataset
human_scaled <- dplyr::select(human_scaled, -Life.Exp)
#add the new categorical value to scaled data
human_scaled <- data.frame(human_scaled, Lifeexp)
str(human_scaled)
```
I will also try to make predictions with the LDA model. To prove the viability of the model, I can split my data into as train (80% of observations) and a test set (20% of observations). The model will be trained with the train set and verified with the test set as a control. Therefore, I am splitting the dataset.
```{r train test}
n<- nrow(human_scaled)
#choose randomly 80% of the rows
ind <-sample(n, size = n*0.8)
#create train set
train <- human_scaled[ind,]
#create test set
test <- human_scaled[-ind,]
# save the correct classes from the test data
correct_classes <- test$Lifeexp
#remove the Lifeexp variable from the test data
test <- dplyr::select(test, -Lifeexp)
```

##3.2 Fitting the LDA model

```{r lda}
lda.fit <- lda(Lifeexp ~ ., data  =train)
#printing the lda.fit object
lda.fit
```
```{r lda arrows}
#lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "deeppink2", tex = 1.25, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$Lifeexp)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```

## 3.3 Making prediction with the LDA model
With our trained LDA-model I can make predictions about new observations for the variable Lifeexp. I predict the classes with the LDA model for the observations of the test set I created earlier. In the cross tabulation we can see how the results of our predictions match with the Lifeexp categories from the test set. 21 observations were correctly predicted and 10 were false. That means our training error is about 32,35%. The model can be used to make predictions, but they are not very accurate. It would be recommendable to leave some of the variables out, like for example percentage of female reprasentatives in parliament as the there is no strong correlation with the variable Lifeexp.
```{r predict with lda}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```




























