In this assignment I am going to fit a multiple regression model on the dataset human and I will try to make predictions from it.
Furthermore I will conduct linear discriminant analysis (LDA) with the data, train the LDA-model and try the model on a testing data set.

#1. The dataset "human"
The dataset originates from the [United Nationa Development Programme (UNDP)](http://hdr.undp.org/en/content/human-development-index-hdi). The UNDP aims to compare the development of countries and regions of the world, including not only growth alone, but many different variables. In this exercise we are using a subset, that I created from the original data. I selected certain variables concerning health and education as well as the Human Development Index (HDI) and the Gross National Income per capita (GNI). Missing values were removed from the dataset. The file of the data wrangling can be found [here](https://github.com/evaroth/IODS-final/blob/master/datawrangling.R). 

The dataset contains 155 observations of 8 variables. The observations are made for 155 countries. Out of the 8 variables GNI and Mat.Mor are integer variables, the others are numerical. In the following list you may see the names of the variables and what kind of information they have recorded.

* **HDI:** human development index
* **Edu2.FM:** ratio of women with at least secondary education to men with at least seconday reducation
* **Life.Exp:** life expectancy at birth
* **Mean.Edu:** mean years of education
* **GNI:** gross national income per capita
* **Mat.Mor:** maternal mortality ratio
* **Ado.Birth:** adolescent birth rate
* **Parli.F:** percentage of female representatives in parliament

I hypothesize, that variables concerning education (Edu.FM, Mean.Edu, Parli.F) have a strong influence on variables concerning health (Life.Exp, Mat.Mor, Parli.F). A better education equally for men and women, has eventually a better influence on life expectancy and health condition of a people.

```{r data human}
#At first I am accessing all the packages that I will be using during this exercise
library(GGally)
library(ggplot2)
library(MASS)
library(corrplot)
#Now we read the data
human <- read.csv("/Users/eva-mariaroth/Documents/GitHub/IODS-final/human_.csv", header = TRUE, sep= ",")
#Exploring the data, its structur and dimensions
str(human)
```
I want to take a closer look at the variables and their relationships to each other. In order to do this I can pair the variables and see if there are correlations.The plot matrix shows us the distribution of the data for each variable. The variables are paired and the results are shown graphically in scatter plots and with the Correlation Coefficient. We can find the highest correlation between Life Expectancy and HDI with a correlation coefficient of 0.905.
```{r human pairs}
p <- ggpairs(human, mapping = aes(col = "green4", alpha = 0.7), lower = list(combo = wrap("facethist", bins = 18)), title = "Fig. 1: Graphical overview of the dataset human")
p
```

Another way to get a quick overview about the correlation of the variables is the correlation plot.In the correlation plot the colour indicates what kind of correlation we have:    
red - negative correlation  
blue - positive correlation  
The size of the cirlce and the intensity of the colour indicate the correlation coefficient.  
strong colours - corrolation coefficient is high    
light colours  - corrolation coefficient is low 
We can see that the the variables are strongly correlating with each other, except for the amount of female representatives in the parliament. It does not seem to have a significant influence on HDI, GNI or Life Expectancy. Obviously other factors play a bigger role here. There are strong positive correlations between HDI, Life Expectancy and Mean Education. Strong negative correlations can be found between HDI and Maternal mortality ratio as well as life expectancy and maternal mortality ratio.

```{r corrplot}
cor_matrix<-cor(human)
corrplot(cor_matrix, method="circle", type = "lower", cl.pos = "b", tl.pos = "d", tl.cex = 0.6, title = "
         Fig.2: Correlation plot for the paired dataset of human")
```

# 2. Regression analysis
##2.1 Simple regression
In order to find out more about the impact of education on health we will perform a regression analysis for the varibales Mean.Edu and Life.Exp, the two variables, very simplified, standing for education and health. In Figure 3 we can see the observations of theses variables plotted against each other and a linear regression line fitted to them. 
```{r simple regr scatterplot}
#creating a scatter plot of Life.Exp versus Mean.Edu with a linear regression line
qplot(Life.Exp, Mean.Edu, data = human, main = "Fig.3: Linear regression for life expectancy and mean years of education", colour = "red") + geom_smooth(method = "lm")
```

Now we want to write a simple regression model and see what kind of information it gives us.Life.Exp beeing the dependant variable y and Mean.Edu the explanatory variable x. Alpha indicates the point where the regression line intercepts the y-axis and beta to the slope of the regression line.Epsilon is an unobservable random variable, that is assumed to add noise to the observations and describes the errors of the model.

![](/Users/eva-mariaroth/Desktop/Bildschirmfoto 2017-12-18 um 11.59.50.png)
The summary of the simple regression model at first shows us the formula applied, indicating the dependant variable and the explanatory variable. Then it gives us a summary of the residuals of the model. The residuals describe the prediction errors, i.e. the difference between the predicitions and the actual values. The Estimate of the Intercept gives us the alpha parameter. In this case it would be 55.46. The estimate for the variable Mean.Edu gives us the beta parameter: 1,95. The P value of the model, based on the t-test.

```{r simple regr}
#fitting a linear model where Life.Exp is the target, whereas mean. edu is the explanatory variable
my_model <- lm(Life.Exp ~ Mean.Edu, data = human)
summary(my_model)
```

###2.2 Making predictions using the model
```{r predict}
#predicting
m <- lm(Life.Exp ~ Mean.Edu, data = human)

# print out a summary of the model
summary(m)

# New observations
new_mean.edu <- c("Niger" = 12.1, "Chad"= 11.5, "Mali" = 10.3, "Nepal" = 10.5, "Haiti" =11,7, "Yemen" = 10)
new_data <- data.frame(Mean.Edu = new_mean.edu)

# Print out the new data
new_data
predict(m, newdata = new_data)
```




# 2.3 Multiple regression model
For taken more variables into account, we can write a multiple regression model.

```{r multipl regr}
#fitting a multiple regression model with several explanatory variables
my_model2 <- lm(Life.Exp ~ Mean.Edu + Edu2.FM + Parli.F, data =human)
summary(my_model2)
```
# 2.4 Validity check for both models
```{r validity}
#exploring validity of the model by drawing diagnostic plots
par(mfrow = c(2,2))
plot(my_model2, which = c(1,2,5))
```

```{r diagnostic2}
par(mfrow = c(2,2))
plot(my_model, which = c(1,2,5))
```
# 4 Performing Linear Discriminant analysis
```{r scaling}
#keep only the columns regarding health and education
human2 <- read.csv("/Users/eva-mariaroth/Documents/GitHub/IODS-final/human2.csv", header = TRUE, sep= ",")
human_scaled <- scale(human2)
# summaries of the scaled variables
summary(human_scaled)
```
```{r class}
# class of the boston_scaled object
class(human_scaled)

# change the object to data frame
human_scaled <- as.data.frame(human_scaled)
summary(human_scaled$Life.Exp)
bins <- quantile(human_scaled$Life.Exp)
bins
#create a categorical variable crime
Lifeexp <- cut(human_scaled$Life.Exp, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
table(Lifeexp)
```
```{r train test.}
#remove original Life.Exp variable from the dataset
human_scaled <- dplyr::select(human_scaled, -Life.Exp)
#add the new categorical value to scaled data
human_scaled <- data.frame(human_scaled, Lifeexp)
str(human_scaled)
```
```{r train test}
n<- nrow(human_scaled)
#choose randomly 80% of the rows
ind <-sample(n, size = n*0.8)
#create train set
train <- human_scaled[ind,]
#create test set
test <- human_scaled[-ind,]
# save the correct classes from the test data
correct_classes <- test$Lifeexp
#remove the Lifeexp variable from the test data
test <- dplyr::select(test, -Lifeexp)
```

###linear discriminant analysis

```{r lda}
lda.fit <- lda(Lifeexp ~ ., data  =train)
#printing the lda.fit object
lda.fit
```
```{r lda arrows}
#lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "deeppink2", tex = 1.25, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$Lifeexp)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2.5)
```

#Predicting
```{r predict with lda}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```




























